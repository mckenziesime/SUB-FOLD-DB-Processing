{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7833f0d7-ceaa-4f44-92dc-72486357e495",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## **Reformatting Interpreter Observations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2bdff8-221e-4ac7-9e3d-d0fb7e73ef77",
   "metadata": {},
   "source": [
    "    This script converts all DB files from TSPO to CSV files. They are then combined based on the table that they are in, resulting in three CSV files: Display Table, Event Table, and Polygon Table. It then reformats the Event Table file (separating out the loss/stable/growth probabilities for each epoch) and attaches to it the point information from the Polygon Table file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012ba22-11fa-4b21-8f60-e0b8c2a2825a",
   "metadata": {},
   "source": [
    "##### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fa2d85-67c3-4046-9730-41810e54e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315ab7e-66f9-4aac-9968-33458bb9d855",
   "metadata": {},
   "source": [
    "##### Define path to the folder that contains the .db files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a803be-8d6f-45e1-9466-4c188759dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'C:/Users/McKenzie/Documents/OSU_Masters/intPoints/intPointsDEMO3THESIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54bec11-0355-4498-8ac1-21866f88aef5",
   "metadata": {},
   "source": [
    "#### Converting DB to CSV and combines by table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f5e7c-8650-4542-89c7-384c7b6372a1",
   "metadata": {},
   "source": [
    "##### Function: Converts DB to CSV and merges each table into its own individual file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e15f2e-7778-45f6-928b-d14552b29bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite_to_csv(input_folder):\n",
    "    # A dictionary to keep track of dataframes for each table across all databases\n",
    "    combined_dataframes = {}\n",
    "    db_count = 0  # Counter to track how many .db files were processed\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.db'):\n",
    "            db_file = os.path.join(input_folder, filename)\n",
    "\n",
    "            # Extract the epoch number from the filename using regex\n",
    "            epoch_match = re.search(r'epoch(\\d{4})', filename)\n",
    "            epoch_number = epoch_match.group(1) if epoch_match else 'Unknown'\n",
    "\n",
    "            try:\n",
    "                # Connect to the SQLite database\n",
    "                conn = sqlite3.connect(db_file)\n",
    "                db_count += 1  # Increment the count for every processed .db file\n",
    "                \n",
    "                # Get the list of tables\n",
    "                query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "                tables = pd.read_sql_query(query, conn)\n",
    "                \n",
    "                if tables.empty:\n",
    "                    print(f\"No tables found in {db_file}.\")\n",
    "                    continue\n",
    "                \n",
    "                # Loop through each table and store data in memory\n",
    "                for table_name in tables['name']:\n",
    "                    try:\n",
    "                        # Read the table into a DataFrame\n",
    "                        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "                        \n",
    "                        if df.empty:\n",
    "                            print(f\"Table {table_name} in {db_file} is empty, skipping.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Add the epoch number as a new column\n",
    "                        df['epoch'] = epoch_number\n",
    "\n",
    "                        # Append the DataFrame to the appropriate list in the dictionary\n",
    "                        if table_name not in combined_dataframes:\n",
    "                            combined_dataframes[table_name] = []\n",
    "                        combined_dataframes[table_name].append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing table {table_name} in {db_file}: {e}\")\n",
    "                \n",
    "                # Close the database connection\n",
    "                conn.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error connecting to database {db_file}: {e}\")\n",
    "\n",
    "    # Print the total number of .db files processed\n",
    "    print(f\"Total number of .db files processed: {db_count}\")\n",
    "\n",
    "    # Now, combine all dataframes for each table and save to single CSV files\n",
    "    for table_name, dfs in combined_dataframes.items():\n",
    "        if dfs:\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "            combined_df.to_csv(f\"{table_name}_combined.csv\", index=False)\n",
    "            print(f\"Combined data for {table_name} has been saved to {table_name}_combined.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662bea9-f807-48b4-9b64-3069a2af9eed",
   "metadata": {},
   "source": [
    "##### Applying the above function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ca92f3-22aa-4682-acbf-62e981fa38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of .db files processed: 61\n",
      "Combined data for displayTable has been saved to displayTable_combined.csv.\n",
      "Combined data for eventTable has been saved to eventTable_combined.csv.\n",
      "Combined data for polygonTable has been saved to polygonTable_combined.csv.\n"
     ]
    }
   ],
   "source": [
    "# Convert all DB files in folder to CSV and combine by table. \n",
    "sqlite_to_csv(input_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec186d75-afc9-45d1-94ef-e544558a573d",
   "metadata": {},
   "source": [
    "#### Reformatting the event table file (contains interpreter observations).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebd12d-19ec-4ca4-9a97-2d1ad78ff32e",
   "metadata": {},
   "source": [
    "##### Combining the spectral progression and spectral capacity columns. \n",
    "    Initially we were calling this uncertainty factor \"progression\" but this was switched to \"capacity\" part way through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27f122e-ff40-4924-ac41-6366d361d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_etCombo = pd.read_csv('eventTable_combined.csv')\n",
    "\n",
    "# Combine 'Spectal_Progression' into 'Spectral_Capacity'\n",
    "df_etCombo['Spectral_Capacity'] = df_etCombo['Spectal_Capacity'].combine_first(df_etCombo['Spectal_Progression'])\n",
    "\n",
    "# Drop the 'Spectal_Progression' column\n",
    "df_etCombo.drop('Spectal_Progression', axis=1, inplace=True)\n",
    "df_etCombo.drop('Spectal_Capacity', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d87da-938d-4051-94d2-d296b99aa1a1",
   "metadata": {},
   "source": [
    "##### Removing rows with no data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804721c9-f5e5-4536-b27b-5337163dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'id' is an integer\n",
    "df_etCombo['id'] = df_etCombo['id'].astype(int)\n",
    "\n",
    "# Trim spaces and adjust for case sensitivity in 'plotId'\n",
    "df_etCombo['plotId'] = df_etCombo['plotId'].str.strip().str.lower()\n",
    "\n",
    "# Filter out rows where 'id' is 1 and 'plotId' is 'nodata'\n",
    "df_etCombo = df_etCombo[~((df_etCombo['id'] == 1) & (df_etCombo['plotId'] == 'no data'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb6535-c8ab-4273-871a-8bd5dc29001e",
   "metadata": {},
   "source": [
    "##### Adding a column for user number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9991795-b1db-41c5-a895-ed4e6a5f05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the \"Valid_Name\" column is treated as a string\n",
    "df_etCombo['Valid_Name'] = df_etCombo['Valid_Name'].astype(str)\n",
    "\n",
    "# Mapping from Valid_Name to assigned number\n",
    "name_to_number = {\n",
    "    '934367363': 7, 'andrewcr': 1, 'asdf': 7, 'cahillem': 2, 'janesk': 4,\n",
    "    'Nicholas': 3, 'sawyerre': 5, 'shinzaas': 6, 'Stable plot seen in all data.': 7,\n",
    "    'underhik': 7, 'nan': 7\n",
    "}\n",
    "\n",
    "# Default number for blank names\n",
    "default_number = 7\n",
    "\n",
    "# Function to get the number from Valid_Name\n",
    "def get_number(name):\n",
    "    if name.strip() == '':  # Check for blank names\n",
    "        return default_number\n",
    "    return name_to_number.get(name, default_number)\n",
    "\n",
    "# Populate the 'user' column\n",
    "df_etCombo['user'] = df_etCombo['Valid_Name'].apply(get_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d08bd-51d6-41e0-aa33-4af28069122f",
   "metadata": {},
   "source": [
    "##### Removing repeat observations that are due to data entry errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8207dbeb-d426-43eb-ab0f-6672444aea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker numbers of the deleted rows: ['87950922' '81395819' '84152401' '66411237' '29398292' '97187302'\n",
      " '76577468' '18472705' '7581613' '10351455' '67290870' '40107720'\n",
      " '23179987' '58835211' '91186758' '96506508' '76927573' '69839659'\n",
      " '97765910' '19195268' '10265774' '19398624' '31920893' '65723809'\n",
      " '15723160' '43497672' '12493287' '48460786' '74866697' '45386448'\n",
      " '18389439' '58326896' '42212153' '39463480' '29895800' '28326592'\n",
      " '10586330' '33184069' '19048534' '40616112' '67226290' '95590134'\n",
      " '9642087' '21375171' '28828997' '17117791' '82492414' '42526660'\n",
      " '81626876' '66982560' '99655003' '36076168' '93051693' '91205046'\n",
      " '21431604' '59273721' '40266624' '6603721' '39237506' '54528681'\n",
      " '22351213' '24943628' '74702469' '69412785' '61499310' '48769782'\n",
      " '45782398' '49011078' '49975472' '30898710' '45904836' '28259348'\n",
      " '68946576' '60374549' '93089310' '20756865' '8515248' '41273984'\n",
      " '97216086' '16382155' '74126659' '25821022' '89864575' '43059704'\n",
      " '71752994' '22313675' '42237723' '26696384' '21164143' '62560436'\n",
      " '2267151' '72618363' '45534945' '6215777' '64335364' '85374678'\n",
      " '86186120' '25279485' '39326583' '9718985' '8950428' '99822939'\n",
      " '34630297' '66524428' '29726193' '2761268' '16948733' '82561653'\n",
      " '95229826' '56045257' '35028291' '83680254' '68862957' '76751542'\n",
      " '60428320' '71855026' '42921710' '17564098' '90647233' '14802016'\n",
      " '86112033' '6378965' '55227732' '96257518' '62934649' '99130594'\n",
      " '83184471' '33658646' '36952705' '39838597' '3076056' '29891893'\n",
      " '56513038' '50859636' '38663071' '79829791' '89095063' '34423062'\n",
      " '17786177' '41552683' '5179832' '33437571' '51259035' '66844677'\n",
      " '3203974' '45505757' '43980510' '24076220' '67789724' '59040681'\n",
      " '55415834' '2964911' '9511848' '52646376' '82165870' '95870061'\n",
      " '64726989' '83663228' '37795995' '29231984' '25708260' '23200963'\n",
      " '87116764' '2166808' '36294526' '98008296' '96700887' '54197206'\n",
      " '84532448' '69496397' '43432041' '1114270' '16218512' '77524298'\n",
      " '98056564' '72024350' '41422436' '63228425' '42314721' '69890479'\n",
      " '73118339' '20274120' '87763548' '62896501' '32285400' '22291674'\n",
      " '82911264' '79382632' '3017566' '57334075' '63213362' '31271106'\n",
      " '2936356' '90100681' '68746478' '8993709' '81618207' '12864094'\n",
      " '15835215' '11481168' '60036640' '89061417' '75054900' '8017112'\n",
      " '42161534' '47125303' '60026525' '31048680' '51837672' '17991182'\n",
      " '48951984' '65393372' '26592522' '6364455' '70471192' '4818226'\n",
      " '20386056' '94110607' '57077709' '47250339' '62237414' '71188942'\n",
      " '94935464' '19767862' '86463240' '22921697' '52605574' '32512659'\n",
      " '87299580' '29463739' '6876491' '2766303' '12356258' '99051244'\n",
      " '94548779' '14987' '47787167' '10330467' '45403133' '15016681' '73002240'\n",
      " '65584022']\n",
      "Count of the deleted rows: 291\n"
     ]
    }
   ],
   "source": [
    "# Identify duplicate rows based on 'user', 'tracker', and 'epoch'\n",
    "duplicate_rows = df_etCombo[df_etCombo.duplicated(subset=['user', 'tracker', 'epoch'], keep='first')]\n",
    "\n",
    "# Extract the 'tracker' numbers of the deleted rows\n",
    "deleted_tracker_numbers = duplicate_rows['tracker'].unique()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "deleted_rows_count = len(duplicate_rows)\n",
    "\n",
    "# Print the tracker numbers and the count of the deleted rows\n",
    "print(\"Tracker numbers of the deleted rows:\", deleted_tracker_numbers)\n",
    "print(\"Count of the deleted rows:\", deleted_rows_count)\n",
    "\n",
    "# Now remove the duplicate rows, keeping only the first occurrence\n",
    "df_etCombo = df_etCombo.drop_duplicates(subset=['user', 'tracker', 'epoch'], keep='first')\n",
    "\n",
    "# Optional: Reset index after removing duplicates\n",
    "df_etCombo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56678b82-d225-4a2f-bf56-46aafff92c26",
   "metadata": {},
   "source": [
    "#### Creating and populating the probability of loss/stable/growth columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fcee7-05ce-4d8b-aee7-02957931eb73",
   "metadata": {},
   "source": [
    "##### Get the number of epochs from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449cdb78-2ad2-4052-80e2-2979af9f64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the DataFrame: 2687\n",
      "Epoch List: [1, 2, 3, 4, 5, 6, 7, 8, 9, 9999]\n",
      "Tracker numbers with more than 2 occurrences: ['55400962' '94123684' '74866697']\n",
      "Number of Times Unique User-Tracker Combos Repeated:\n",
      "1 times: 2544 combinations\n",
      "2 times: 67 combinations\n",
      "3 times: 3 combinations\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of rows in the DataFrame\n",
    "total_rows = df_etCombo.shape[0]\n",
    "print(f\"Total number of rows in the DataFrame: {total_rows}\")\n",
    "\n",
    "# Extract unique numbers from the \"epoch\" column\n",
    "epoch_list = df_etCombo[\"epoch\"].unique().tolist()\n",
    "print(\"Epoch List:\", epoch_list)\n",
    "\n",
    "# Ensure the reEval column is properly converted to integers for comparison\n",
    "df_etCombo[\"reEval\"] = pd.to_numeric(df_etCombo[\"reEval\"], errors=\"coerce\")\n",
    "\n",
    "# Group by 'user' and 'tracker' and count the occurrences of each combination\n",
    "user_tracker_counts = df_etCombo.groupby([\"user\", \"tracker\"]).size()\n",
    "\n",
    "# Identify tracker numbers for combinations appearing more than 2 times\n",
    "trackers_over_2 = user_tracker_counts[user_tracker_counts > 2].reset_index()\n",
    "trackers_over_2_list = trackers_over_2['tracker'].unique()\n",
    "\n",
    "# Print tracker numbers with more than 2 occurrences\n",
    "if len(trackers_over_2_list) > 0:\n",
    "    print(f\"Tracker numbers with more than 2 occurrences: {trackers_over_2_list}\")\n",
    "else:\n",
    "    print(\"No tracker numbers with more than 2 occurrences.\")\n",
    "\n",
    "# Count how many times each repetition occurred (e.g., 2 times, 3 times, etc.)\n",
    "repetition_counts = user_tracker_counts.value_counts().sort_index()\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of Times Unique User-Tracker Combos Repeated:\")\n",
    "for times, count in repetition_counts.items():\n",
    "    print(f\"{times} times: {count} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b99779-1600-46c1-90cb-374136389d22",
   "metadata": {},
   "source": [
    "##### Add the necessary columns to eventually compress the unique user/point combinations into a single line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536cfbeb-a2d9-417e-939b-78ea5beff397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Columns: ['E1_Prob_4month_Loss', 'E1_Prob_4month_Stable', 'E1_Prob_4month_Growth', 'E1_Prob_6month_Loss', 'E1_Prob_6month_Stable', 'E1_Prob_6month_Growth', 'E1_Prob_12month_Loss', 'E1_Prob_12month_Stable', 'E1_Prob_12month_Growth', 'E2_Prob_4month_Loss', 'E2_Prob_4month_Stable', 'E2_Prob_4month_Growth', 'E2_Prob_6month_Loss', 'E2_Prob_6month_Stable', 'E2_Prob_6month_Growth', 'E2_Prob_12month_Loss', 'E2_Prob_12month_Stable', 'E2_Prob_12month_Growth', 'E3_Prob_4month_Loss', 'E3_Prob_4month_Stable', 'E3_Prob_4month_Growth', 'E3_Prob_6month_Loss', 'E3_Prob_6month_Stable', 'E3_Prob_6month_Growth', 'E3_Prob_12month_Loss', 'E3_Prob_12month_Stable', 'E3_Prob_12month_Growth', 'E4_Prob_4month_Loss', 'E4_Prob_4month_Stable', 'E4_Prob_4month_Growth', 'E4_Prob_6month_Loss', 'E4_Prob_6month_Stable', 'E4_Prob_6month_Growth', 'E4_Prob_12month_Loss', 'E4_Prob_12month_Stable', 'E4_Prob_12month_Growth', 'E5_Prob_4month_Loss', 'E5_Prob_4month_Stable', 'E5_Prob_4month_Growth', 'E5_Prob_6month_Loss', 'E5_Prob_6month_Stable', 'E5_Prob_6month_Growth', 'E5_Prob_12month_Loss', 'E5_Prob_12month_Stable', 'E5_Prob_12month_Growth', 'E6_Prob_4month_Loss', 'E6_Prob_4month_Stable', 'E6_Prob_4month_Growth', 'E6_Prob_6month_Loss', 'E6_Prob_6month_Stable', 'E6_Prob_6month_Growth', 'E6_Prob_12month_Loss', 'E6_Prob_12month_Stable', 'E6_Prob_12month_Growth', 'E7_Prob_4month_Loss', 'E7_Prob_4month_Stable', 'E7_Prob_4month_Growth', 'E7_Prob_6month_Loss', 'E7_Prob_6month_Stable', 'E7_Prob_6month_Growth', 'E7_Prob_12month_Loss', 'E7_Prob_12month_Stable', 'E7_Prob_12month_Growth', 'E8_Prob_4month_Loss', 'E8_Prob_4month_Stable', 'E8_Prob_4month_Growth', 'E8_Prob_6month_Loss', 'E8_Prob_6month_Stable', 'E8_Prob_6month_Growth', 'E8_Prob_12month_Loss', 'E8_Prob_12month_Stable', 'E8_Prob_12month_Growth', 'E9_Prob_4month_Loss', 'E9_Prob_4month_Stable', 'E9_Prob_4month_Growth', 'E9_Prob_6month_Loss', 'E9_Prob_6month_Stable', 'E9_Prob_6month_Growth', 'E9_Prob_12month_Loss', 'E9_Prob_12month_Stable', 'E9_Prob_12month_Growth', 'E9999_Prob_4month_Loss', 'E9999_Prob_4month_Stable', 'E9999_Prob_4month_Growth', 'E9999_Prob_6month_Loss', 'E9999_Prob_6month_Stable', 'E9999_Prob_6month_Growth', 'E9999_Prob_12month_Loss', 'E9999_Prob_12month_Stable', 'E9999_Prob_12month_Growth']\n"
     ]
    }
   ],
   "source": [
    "# Define the naming pattern for the new columns\n",
    "months = [4, 6, 12]\n",
    "conditions = [\"Loss\", \"Stable\", \"Growth\"]\n",
    "\n",
    "# Generate column names based on the \"epoch\" numbers\n",
    "new_columns = []\n",
    "for epoch_num in epoch_list:\n",
    "    for month in months:\n",
    "        for condition in conditions:\n",
    "            column_name = f\"E{epoch_num}_Prob_{month}month_{condition}\"\n",
    "            new_columns.append(column_name)\n",
    "            df_etCombo[column_name] = np.nan  # Initialize with NaN\n",
    "\n",
    "print(\"Added Columns:\", new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755ac09-b6ac-4000-85f8-2b4b42e5f018",
   "metadata": {},
   "source": [
    "##### Populate the new columns based on the values in the tracker, user, epoch, reEval, and probability columns so that each unique user/tracker combo has only one row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c57b663-2a07-4534-bd23-257975106f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract JSON values and convert them to percentages\n",
    "def extract_json_values(json_str):\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "        return {\n",
    "            \"Loss\": int(json_data.get(\"loss\", \"0%\").replace(\"%\", \"\")),\n",
    "            \"Stable\": int(json_data.get(\"Stable\", \"0%\").replace(\"%\", \"\")),\n",
    "            \"Growth\": int(json_data.get(\"Growth\", \"0%\").replace(\"%\", \"\"))\n",
    "        }\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {\"Loss\": 0, \"Stable\": 0, \"Growth\": 0}\n",
    "\n",
    "# Iterate through the DataFrame and apply logic based on 'reEval' column\n",
    "for index, row in df_etCombo.iterrows():\n",
    "    epoch_num = row[\"epoch\"]  # Get the epoch number for the current row\n",
    "\n",
    "    # If 'reEval' == 0, populate the correct epoch columns directly\n",
    "    if row[\"reEval\"] == 0:\n",
    "        for month in [4, 6, 12]:\n",
    "            for condition in [\"Loss\", \"Stable\", \"Growth\"]:\n",
    "                column_name = f'E{epoch_num}_Prob_{month}month_{condition}'\n",
    "                df_etCombo.at[index, column_name] = extract_json_values(\n",
    "                    row[f\"Probability_Forest_Loss_{month}_month\"]\n",
    "                )[condition]\n",
    "\n",
    "    elif row[\"reEval\"] == 1:\n",
    "        # Search for the matching row with the same tracker, user, and 'reEval' == 0\n",
    "        match = df_etCombo[\n",
    "            (df_etCombo[\"tracker\"] == row[\"tracker\"]) &\n",
    "            (df_etCombo[\"user\"] == row[\"user\"]) &\n",
    "            (df_etCombo[\"reEval\"] == 0)\n",
    "        ]\n",
    "\n",
    "        if not match.empty:\n",
    "            # If a matching row is found, copy the values to the matched row\n",
    "            match_index = match.index[0]  # Assume only one matching row\n",
    "\n",
    "            for month in [4, 6, 12]:\n",
    "                json_values = extract_json_values(row[f\"Probability_Forest_Loss_{month}_month\"])\n",
    "\n",
    "                for condition, value in json_values.items():\n",
    "                    target_column = f'E{epoch_num}_Prob_{month}month_{condition}'\n",
    "                    df_etCombo.at[match_index, target_column] = value\n",
    "\n",
    "            # Mark the current row for deletion\n",
    "            df_etCombo.at[index, \"delete_flag\"] = True\n",
    "        else:\n",
    "            # If no matching row is found, populate the epoch-specific columns for this row\n",
    "            for month in [4, 6, 12]:\n",
    "                json_values = extract_json_values(row[f\"Probability_Forest_Loss_{month}_month\"])\n",
    "\n",
    "                for condition, value in json_values.items():\n",
    "                    target_column = f'E{epoch_num}_Prob_{month}month_{condition}'\n",
    "                    df_etCombo.at[index, target_column] = value\n",
    "\n",
    "# Delete rows with 'reEval' == 1 that had matching rows\n",
    "df_etCombo = df_etCombo[df_etCombo[\"delete_flag\"].isna()]\n",
    "\n",
    "# Clean up by dropping the 'delete_flag' column\n",
    "df_etCombo.drop(columns=[\"delete_flag\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b013be12-d567-455b-973a-3d054f6d5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the DataFrame: 2617\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of rows in the DataFrame\n",
    "total_rows = df_etCombo.shape[0]\n",
    "print(f\"Total number of rows in the DataFrame: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8070be8-2ecc-45fe-9698-7ea20c30987b",
   "metadata": {},
   "source": [
    "##### Delete the unecessary columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "333dc8cc-0734-4bf0-a834-56aeb7e81208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted columns: ['Probability_Forest_Loss_4_month', 'Probability_Forest_Loss_6_month', 'Probability_Forest_Loss_12_month', 'epoch']\n",
      "Remaining columns: ['id', 'plotId', 'Valid_Date', 'LT_YOD', 'Image_Condition', 'Spatial_Context', 'Valid_Name', 'Comment', 'User_IP', 'user_elapsed_time', 'tracker', 'reEval', 'Spectral_Capacity', 'user', 'E1_Prob_4month_Loss', 'E1_Prob_4month_Stable', 'E1_Prob_4month_Growth', 'E1_Prob_6month_Loss', 'E1_Prob_6month_Stable', 'E1_Prob_6month_Growth', 'E1_Prob_12month_Loss', 'E1_Prob_12month_Stable', 'E1_Prob_12month_Growth', 'E2_Prob_4month_Loss', 'E2_Prob_4month_Stable', 'E2_Prob_4month_Growth', 'E2_Prob_6month_Loss', 'E2_Prob_6month_Stable', 'E2_Prob_6month_Growth', 'E2_Prob_12month_Loss', 'E2_Prob_12month_Stable', 'E2_Prob_12month_Growth', 'E3_Prob_4month_Loss', 'E3_Prob_4month_Stable', 'E3_Prob_4month_Growth', 'E3_Prob_6month_Loss', 'E3_Prob_6month_Stable', 'E3_Prob_6month_Growth', 'E3_Prob_12month_Loss', 'E3_Prob_12month_Stable', 'E3_Prob_12month_Growth', 'E4_Prob_4month_Loss', 'E4_Prob_4month_Stable', 'E4_Prob_4month_Growth', 'E4_Prob_6month_Loss', 'E4_Prob_6month_Stable', 'E4_Prob_6month_Growth', 'E4_Prob_12month_Loss', 'E4_Prob_12month_Stable', 'E4_Prob_12month_Growth', 'E5_Prob_4month_Loss', 'E5_Prob_4month_Stable', 'E5_Prob_4month_Growth', 'E5_Prob_6month_Loss', 'E5_Prob_6month_Stable', 'E5_Prob_6month_Growth', 'E5_Prob_12month_Loss', 'E5_Prob_12month_Stable', 'E5_Prob_12month_Growth', 'E6_Prob_4month_Loss', 'E6_Prob_4month_Stable', 'E6_Prob_4month_Growth', 'E6_Prob_6month_Loss', 'E6_Prob_6month_Stable', 'E6_Prob_6month_Growth', 'E6_Prob_12month_Loss', 'E6_Prob_12month_Stable', 'E6_Prob_12month_Growth', 'E7_Prob_4month_Loss', 'E7_Prob_4month_Stable', 'E7_Prob_4month_Growth', 'E7_Prob_6month_Loss', 'E7_Prob_6month_Stable', 'E7_Prob_6month_Growth', 'E7_Prob_12month_Loss', 'E7_Prob_12month_Stable', 'E7_Prob_12month_Growth', 'E8_Prob_4month_Loss', 'E8_Prob_4month_Stable', 'E8_Prob_4month_Growth', 'E8_Prob_6month_Loss', 'E8_Prob_6month_Stable', 'E8_Prob_6month_Growth', 'E8_Prob_12month_Loss', 'E8_Prob_12month_Stable', 'E8_Prob_12month_Growth', 'E9_Prob_4month_Loss', 'E9_Prob_4month_Stable', 'E9_Prob_4month_Growth', 'E9_Prob_6month_Loss', 'E9_Prob_6month_Stable', 'E9_Prob_6month_Growth', 'E9_Prob_12month_Loss', 'E9_Prob_12month_Stable', 'E9_Prob_12month_Growth', 'E9999_Prob_4month_Loss', 'E9999_Prob_4month_Stable', 'E9999_Prob_4month_Growth', 'E9999_Prob_6month_Loss', 'E9999_Prob_6month_Stable', 'E9999_Prob_6month_Growth', 'E9999_Prob_12month_Loss', 'E9999_Prob_12month_Stable', 'E9999_Prob_12month_Growth']\n"
     ]
    }
   ],
   "source": [
    "# Delete the specified columns from the DataFrame\n",
    "columns_to_delete = [\n",
    "    \"Probability_Forest_Loss_4_month\", \n",
    "    \"Probability_Forest_Loss_6_month\", \n",
    "    \"Probability_Forest_Loss_12_month\", \n",
    "    \"epoch\"\n",
    "]\n",
    "\n",
    "df_etCombo.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Deleted columns: {columns_to_delete}\")\n",
    "\n",
    "# Print all remaining column names as a list\n",
    "print(\"Remaining columns:\", df_etCombo.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69c553-5332-4c4b-984a-37e267973a20",
   "metadata": {},
   "source": [
    "##### Generating and populating columns for the average probability from that user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d65701-d796-4faa-9724-0fda1e26dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found column: E1_Prob_4month_Loss\n",
      "Found column: E2_Prob_4month_Loss\n",
      "Found column: E3_Prob_4month_Loss\n",
      "Found column: E4_Prob_4month_Loss\n",
      "Found column: E5_Prob_4month_Loss\n",
      "Found column: E6_Prob_4month_Loss\n",
      "Found column: E7_Prob_4month_Loss\n",
      "Found column: E8_Prob_4month_Loss\n",
      "Found column: E9_Prob_4month_Loss\n",
      "Found column: E9999_Prob_4month_Loss\n",
      "Added column 'Ave_Prob_4month_Loss' using columns: ['E1_Prob_4month_Loss', 'E2_Prob_4month_Loss', 'E3_Prob_4month_Loss', 'E4_Prob_4month_Loss', 'E5_Prob_4month_Loss', 'E6_Prob_4month_Loss', 'E7_Prob_4month_Loss', 'E8_Prob_4month_Loss', 'E9_Prob_4month_Loss', 'E9999_Prob_4month_Loss']\n",
      "Found column: E1_Prob_4month_Stable\n",
      "Found column: E2_Prob_4month_Stable\n",
      "Found column: E3_Prob_4month_Stable\n",
      "Found column: E4_Prob_4month_Stable\n",
      "Found column: E5_Prob_4month_Stable\n",
      "Found column: E6_Prob_4month_Stable\n",
      "Found column: E7_Prob_4month_Stable\n",
      "Found column: E8_Prob_4month_Stable\n",
      "Found column: E9_Prob_4month_Stable\n",
      "Found column: E9999_Prob_4month_Stable\n",
      "Added column 'Ave_Prob_4month_Stable' using columns: ['E1_Prob_4month_Stable', 'E2_Prob_4month_Stable', 'E3_Prob_4month_Stable', 'E4_Prob_4month_Stable', 'E5_Prob_4month_Stable', 'E6_Prob_4month_Stable', 'E7_Prob_4month_Stable', 'E8_Prob_4month_Stable', 'E9_Prob_4month_Stable', 'E9999_Prob_4month_Stable']\n",
      "Found column: E1_Prob_4month_Growth\n",
      "Found column: E2_Prob_4month_Growth\n",
      "Found column: E3_Prob_4month_Growth\n",
      "Found column: E4_Prob_4month_Growth\n",
      "Found column: E5_Prob_4month_Growth\n",
      "Found column: E6_Prob_4month_Growth\n",
      "Found column: E7_Prob_4month_Growth\n",
      "Found column: E8_Prob_4month_Growth\n",
      "Found column: E9_Prob_4month_Growth\n",
      "Found column: E9999_Prob_4month_Growth\n",
      "Added column 'Ave_Prob_4month_Growth' using columns: ['E1_Prob_4month_Growth', 'E2_Prob_4month_Growth', 'E3_Prob_4month_Growth', 'E4_Prob_4month_Growth', 'E5_Prob_4month_Growth', 'E6_Prob_4month_Growth', 'E7_Prob_4month_Growth', 'E8_Prob_4month_Growth', 'E9_Prob_4month_Growth', 'E9999_Prob_4month_Growth']\n",
      "Found column: E1_Prob_6month_Loss\n",
      "Found column: E2_Prob_6month_Loss\n",
      "Found column: E3_Prob_6month_Loss\n",
      "Found column: E4_Prob_6month_Loss\n",
      "Found column: E5_Prob_6month_Loss\n",
      "Found column: E6_Prob_6month_Loss\n",
      "Found column: E7_Prob_6month_Loss\n",
      "Found column: E8_Prob_6month_Loss\n",
      "Found column: E9_Prob_6month_Loss\n",
      "Found column: E9999_Prob_6month_Loss\n",
      "Added column 'Ave_Prob_6month_Loss' using columns: ['E1_Prob_6month_Loss', 'E2_Prob_6month_Loss', 'E3_Prob_6month_Loss', 'E4_Prob_6month_Loss', 'E5_Prob_6month_Loss', 'E6_Prob_6month_Loss', 'E7_Prob_6month_Loss', 'E8_Prob_6month_Loss', 'E9_Prob_6month_Loss', 'E9999_Prob_6month_Loss']\n",
      "Found column: E1_Prob_6month_Stable\n",
      "Found column: E2_Prob_6month_Stable\n",
      "Found column: E3_Prob_6month_Stable\n",
      "Found column: E4_Prob_6month_Stable\n",
      "Found column: E5_Prob_6month_Stable\n",
      "Found column: E6_Prob_6month_Stable\n",
      "Found column: E7_Prob_6month_Stable\n",
      "Found column: E8_Prob_6month_Stable\n",
      "Found column: E9_Prob_6month_Stable\n",
      "Found column: E9999_Prob_6month_Stable\n",
      "Added column 'Ave_Prob_6month_Stable' using columns: ['E1_Prob_6month_Stable', 'E2_Prob_6month_Stable', 'E3_Prob_6month_Stable', 'E4_Prob_6month_Stable', 'E5_Prob_6month_Stable', 'E6_Prob_6month_Stable', 'E7_Prob_6month_Stable', 'E8_Prob_6month_Stable', 'E9_Prob_6month_Stable', 'E9999_Prob_6month_Stable']\n",
      "Found column: E1_Prob_6month_Growth\n",
      "Found column: E2_Prob_6month_Growth\n",
      "Found column: E3_Prob_6month_Growth\n",
      "Found column: E4_Prob_6month_Growth\n",
      "Found column: E5_Prob_6month_Growth\n",
      "Found column: E6_Prob_6month_Growth\n",
      "Found column: E7_Prob_6month_Growth\n",
      "Found column: E8_Prob_6month_Growth\n",
      "Found column: E9_Prob_6month_Growth\n",
      "Found column: E9999_Prob_6month_Growth\n",
      "Added column 'Ave_Prob_6month_Growth' using columns: ['E1_Prob_6month_Growth', 'E2_Prob_6month_Growth', 'E3_Prob_6month_Growth', 'E4_Prob_6month_Growth', 'E5_Prob_6month_Growth', 'E6_Prob_6month_Growth', 'E7_Prob_6month_Growth', 'E8_Prob_6month_Growth', 'E9_Prob_6month_Growth', 'E9999_Prob_6month_Growth']\n",
      "Found column: E1_Prob_12month_Loss\n",
      "Found column: E2_Prob_12month_Loss\n",
      "Found column: E3_Prob_12month_Loss\n",
      "Found column: E4_Prob_12month_Loss\n",
      "Found column: E5_Prob_12month_Loss\n",
      "Found column: E6_Prob_12month_Loss\n",
      "Found column: E7_Prob_12month_Loss\n",
      "Found column: E8_Prob_12month_Loss\n",
      "Found column: E9_Prob_12month_Loss\n",
      "Found column: E9999_Prob_12month_Loss\n",
      "Added column 'Ave_Prob_12month_Loss' using columns: ['E1_Prob_12month_Loss', 'E2_Prob_12month_Loss', 'E3_Prob_12month_Loss', 'E4_Prob_12month_Loss', 'E5_Prob_12month_Loss', 'E6_Prob_12month_Loss', 'E7_Prob_12month_Loss', 'E8_Prob_12month_Loss', 'E9_Prob_12month_Loss', 'E9999_Prob_12month_Loss']\n",
      "Found column: E1_Prob_12month_Stable\n",
      "Found column: E2_Prob_12month_Stable\n",
      "Found column: E3_Prob_12month_Stable\n",
      "Found column: E4_Prob_12month_Stable\n",
      "Found column: E5_Prob_12month_Stable\n",
      "Found column: E6_Prob_12month_Stable\n",
      "Found column: E7_Prob_12month_Stable\n",
      "Found column: E8_Prob_12month_Stable\n",
      "Found column: E9_Prob_12month_Stable\n",
      "Found column: E9999_Prob_12month_Stable\n",
      "Added column 'Ave_Prob_12month_Stable' using columns: ['E1_Prob_12month_Stable', 'E2_Prob_12month_Stable', 'E3_Prob_12month_Stable', 'E4_Prob_12month_Stable', 'E5_Prob_12month_Stable', 'E6_Prob_12month_Stable', 'E7_Prob_12month_Stable', 'E8_Prob_12month_Stable', 'E9_Prob_12month_Stable', 'E9999_Prob_12month_Stable']\n",
      "Found column: E1_Prob_12month_Growth\n",
      "Found column: E2_Prob_12month_Growth\n",
      "Found column: E3_Prob_12month_Growth\n",
      "Found column: E4_Prob_12month_Growth\n",
      "Found column: E5_Prob_12month_Growth\n",
      "Found column: E6_Prob_12month_Growth\n",
      "Found column: E7_Prob_12month_Growth\n",
      "Found column: E8_Prob_12month_Growth\n",
      "Found column: E9_Prob_12month_Growth\n",
      "Found column: E9999_Prob_12month_Growth\n",
      "Added column 'Ave_Prob_12month_Growth' using columns: ['E1_Prob_12month_Growth', 'E2_Prob_12month_Growth', 'E3_Prob_12month_Growth', 'E4_Prob_12month_Growth', 'E5_Prob_12month_Growth', 'E6_Prob_12month_Growth', 'E7_Prob_12month_Growth', 'E8_Prob_12month_Growth', 'E9_Prob_12month_Growth', 'E9999_Prob_12month_Growth']\n",
      "\n",
      "Added average columns: ['Ave_Prob_4month_Loss', 'Ave_Prob_4month_Stable', 'Ave_Prob_4month_Growth', 'Ave_Prob_6month_Loss', 'Ave_Prob_6month_Stable', 'Ave_Prob_6month_Growth', 'Ave_Prob_12month_Loss', 'Ave_Prob_12month_Stable', 'Ave_Prob_12month_Growth']\n"
     ]
    }
   ],
   "source": [
    "# Define the column types we need to create average columns for\n",
    "column_types = [\n",
    "    'Prob_4month_Loss', 'Prob_4month_Stable', 'Prob_4month_Growth',\n",
    "    'Prob_6month_Loss', 'Prob_6month_Stable', 'Prob_6month_Growth',\n",
    "    'Prob_12month_Loss', 'Prob_12month_Stable', 'Prob_12month_Growth'\n",
    "]\n",
    "\n",
    "# Track the columns to be added to the DataFrame\n",
    "new_columns_dict = {}\n",
    "\n",
    "# Ensure every epoch number from 'epoch_list' is used for each column type\n",
    "for col_type in column_types:\n",
    "    column_list = []\n",
    "\n",
    "    # Collect all columns for the current column type using the epoch list\n",
    "    for epoch in epoch_list:\n",
    "        col_name = f'E{epoch}_{col_type}'  # Dynamically create column name\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if col_name in df_etCombo.columns:\n",
    "            print(f\"Found column: {col_name}\")\n",
    "            column_list.append(col_name)\n",
    "        else:\n",
    "            print(f\"Missing column: {col_name}\")\n",
    "\n",
    "    # Calculate the average across the selected columns\n",
    "    if column_list:\n",
    "        new_column_name = f'Ave_{col_type}'\n",
    "        new_columns_dict[new_column_name] = df_etCombo[column_list].mean(axis=1)\n",
    "\n",
    "        print(f\"Added column '{new_column_name}' using columns: {column_list}\")\n",
    "    else:\n",
    "        print(f\"No valid columns found for {col_type}\")\n",
    "\n",
    "# Concatenate the new columns to the original DataFrame in one go\n",
    "df_etCombo = pd.concat([df_etCombo, pd.DataFrame(new_columns_dict)], axis=1)\n",
    "\n",
    "# Print the list of all added average columns\n",
    "print(\"\\nAdded average columns:\", list(new_columns_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39df0d4-20a2-44fa-9d80-85bc56759397",
   "metadata": {},
   "source": [
    "##### Generating and populating columns for the most recent observation by that user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e768d822-ae82-4b5e-b627-8382154ca51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added columns: ['Rec_Prob_4month_Loss', 'Rec_Prob_4month_Stable', 'Rec_Prob_4month_Growth', 'Rec_Prob_6month_Loss', 'Rec_Prob_6month_Stable', 'Rec_Prob_6month_Growth', 'Rec_Prob_12month_Loss', 'Rec_Prob_12month_Stable', 'Rec_Prob_12month_Growth']\n"
     ]
    }
   ],
   "source": [
    "# Track the new columns to be added\n",
    "new_columns_dict = {}\n",
    "\n",
    "# Iterate over each column type and create the new \"Rec_\" columns\n",
    "for col_type in column_types:\n",
    "    rec_column = f'Rec_{col_type}'\n",
    "    new_columns_dict[rec_column] = pd.Series([None] * len(df_etCombo))  # Initialize with None\n",
    "\n",
    "    # Iterate over epoch numbers in reverse order to prioritize higher epochs (e.g., E999 first)\n",
    "    for epoch in sorted(epoch_list, reverse=True):\n",
    "        column_name = f'E{epoch}_{col_type}'\n",
    "\n",
    "        # Only attempt to combine if the column exists in the DataFrame\n",
    "        if column_name in df_etCombo.columns:\n",
    "            # Use combine_first to prioritize non-None values from existing columns\n",
    "            new_columns_dict[rec_column] = new_columns_dict[rec_column].combine_first(df_etCombo[column_name])\n",
    "\n",
    "# Add all the new \"Rec_\" columns to the DataFrame at once using pd.concat()\n",
    "df_etCombo = pd.concat([df_etCombo, pd.DataFrame(new_columns_dict)], axis=1)\n",
    "\n",
    "# Print the added columns\n",
    "added_columns = list(new_columns_dict.keys())\n",
    "print(\"\\nAdded columns:\", added_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109758b2-d7a5-4467-adf8-68f8b44a4e8a",
   "metadata": {},
   "source": [
    "##### Creating a 0/1 column for when Loss probability is highest. \n",
    "    This will be treated as your typical loss/no loss interpreter observations that has no associated uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e697598f-308f-4ee7-a21f-fa7f7a82e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added TF columns: ['E1_TF_12month_Loss', 'E1_TF_4month_Loss', 'E1_TF_6month_Loss', 'E2_TF_12month_Loss', 'E2_TF_4month_Loss', 'E2_TF_6month_Loss', 'E3_TF_12month_Loss', 'E3_TF_4month_Loss', 'E3_TF_6month_Loss', 'E4_TF_12month_Loss', 'E4_TF_4month_Loss', 'E4_TF_6month_Loss', 'E5_TF_12month_Loss', 'E5_TF_4month_Loss', 'E5_TF_6month_Loss', 'E6_TF_12month_Loss', 'E6_TF_4month_Loss', 'E6_TF_6month_Loss', 'E7_TF_12month_Loss', 'E7_TF_4month_Loss', 'E7_TF_6month_Loss', 'E8_TF_12month_Loss', 'E8_TF_4month_Loss', 'E8_TF_6month_Loss', 'E9_TF_12month_Loss', 'E9_TF_4month_Loss', 'E9_TF_6month_Loss', 'E9999_TF_12month_Loss', 'E9999_TF_4month_Loss', 'E9999_TF_6month_Loss', 'Ave_TF_12month_Loss', 'Ave_TF_4month_Loss', 'Ave_TF_6month_Loss', 'Rec_TF_12month_Loss', 'Rec_TF_4month_Loss', 'Rec_TF_6month_Loss']\n"
     ]
    }
   ],
   "source": [
    "# Define a pattern to find all month types dynamically\n",
    "pattern = re.compile(r'Prob_(\\d+)month_(Loss|Stable|Growth)')\n",
    "\n",
    "# Extract all unique month durations from column names\n",
    "month_durations = sorted({match.group(1) for col in df_etCombo.columns for match in [pattern.search(col)] if match})\n",
    "\n",
    "# Track the new columns to be added to the DataFrame\n",
    "new_columns_dict = {}\n",
    "\n",
    "# Create the new TF columns using epoch_list + ['Ave', 'Rec']\n",
    "for prefix in [f'E{epoch}' for epoch in epoch_list] + ['Ave', 'Rec']:\n",
    "    for month in month_durations:\n",
    "        # Define the column names for loss, stable, growth, and TF\n",
    "        loss_col = f'{prefix}_Prob_{month}month_Loss'\n",
    "        stable_col = f'{prefix}_Prob_{month}month_Stable'\n",
    "        growth_col = f'{prefix}_Prob_{month}month_Growth'\n",
    "        tf_col = f'{prefix}_TF_{month}month_Loss'\n",
    "\n",
    "        # Check if all required columns exist\n",
    "        if all(col in df_etCombo.columns for col in [loss_col, stable_col, growth_col]):\n",
    "            # Compute the TF column values, handling NaN values properly\n",
    "            tf_values = np.where(\n",
    "                df_etCombo[[loss_col, stable_col, growth_col]].isna().any(axis=1),\n",
    "                np.nan,  # If any of the values is NaN, set TF to NaN\n",
    "                np.where(\n",
    "                    (df_etCombo[loss_col] > df_etCombo[stable_col]) &\n",
    "                    (df_etCombo[loss_col] > df_etCombo[growth_col]),\n",
    "                    1,  # Set TF to 1 if loss is the highest\n",
    "                    0   # Otherwise, set TF to 0\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Store the TF column values in the dictionary\n",
    "            new_columns_dict[tf_col] = pd.Series(tf_values, index=df_etCombo.index)\n",
    "\n",
    "# Add all the new TF columns to the DataFrame at once using pd.concat()\n",
    "new_tf_df = pd.DataFrame(new_columns_dict, index=df_etCombo.index)\n",
    "\n",
    "# Add the new TF columns to the original DataFrame without creating extra rows\n",
    "df_etCombo = pd.concat([df_etCombo, new_tf_df], axis=1)\n",
    "\n",
    "# Print the added TF columns\n",
    "added_tf_columns = list(new_columns_dict.keys())\n",
    "print(\"\\nAdded TF columns:\", added_tf_columns)\n",
    "\n",
    "# Drop rows that only have NaN in the 'tracker' column\n",
    "df_etCombo.dropna(subset=['tracker'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56228d24-895c-4341-a6bd-34f4ea67f4fd",
   "metadata": {},
   "source": [
    "#### Attaching point metadata from Polygon Table to the Event Table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c884052c-bc4f-4f08-b4cd-d1bea8bb2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the polygon table CSV into a DataFrame\n",
    "df_polygon = pd.read_csv('polygonTable_combined.csv')\n",
    "\n",
    "# Ensure the 'tracker' column is of the same type in both DataFrames\n",
    "df_etCombo['tracker'] = df_etCombo['tracker'].astype(str)\n",
    "df_polygon['tracker'] = df_polygon['tracker'].astype(str)\n",
    "\n",
    "# Keeping only the first entry for each 'tracker'\n",
    "df_polygon = df_polygon.drop_duplicates('tracker', keep='first')\n",
    "\n",
    "# Merging again\n",
    "df_etCombo = df_etCombo.merge(df_polygon[['tracker', 'geo', 'json']], on='tracker', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68b4d2-b2d8-4cc3-adf6-0de73adfd8c6",
   "metadata": {},
   "source": [
    "##### Separating out longitude and latitude for GEE upload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ce60b5-97d2-419e-b6de-72bebaf74e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    longitude   latitude\n",
      "0  103.841873  12.127876\n",
      "1  107.085125  12.983627\n",
      "2  104.668876  13.678627\n",
      "3  105.789876  11.563377\n",
      "4  104.229873  14.220876\n"
     ]
    }
   ],
   "source": [
    "# Function to extract longitude and latitude from the 'geo' column\n",
    "def extract_coordinates(geo_json):\n",
    "    try:\n",
    "        # Ensure the input is a string; if not, convert it to a string\n",
    "        if isinstance(geo_json, str):\n",
    "            # Replace single quotes with double quotes to ensure valid JSON\n",
    "            geo_json = geo_json.replace(\"'\", '\"')\n",
    "\n",
    "        # Parse the JSON string\n",
    "        geo_dict = json.loads(geo_json)\n",
    "\n",
    "        # Extract the coordinates assuming the correct structure\n",
    "        coords = geo_dict.get('coordinates', [None, None])\n",
    "        return coords[0], coords[1]\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to extract coordinates and create new columns\n",
    "df_etCombo[['longitude', 'latitude']] = pd.DataFrame(\n",
    "    df_etCombo['geo'].apply(extract_coordinates).tolist(), index=df_etCombo.index\n",
    ")\n",
    "\n",
    "# Change the data type of 'longitude' and 'latitude' to float, handling NaN values\n",
    "df_etCombo['longitude'] = pd.to_numeric(df_etCombo['longitude'], errors='coerce')\n",
    "df_etCombo['latitude'] = pd.to_numeric(df_etCombo['latitude'], errors='coerce')\n",
    "\n",
    "# Drop the 'geo' column\n",
    "df_etCombo.drop(columns=['geo'], inplace=True)\n",
    "\n",
    "# Confirm the output\n",
    "print(df_etCombo[['longitude', 'latitude']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39172aa5-cfd5-4fe0-86fa-13341c86b8eb",
   "metadata": {},
   "source": [
    "##### Getting the stratum ID for checking accuracy later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d869c6dd-3814-4f6d-a58f-c3223e7a5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming \"json\" column to avoid confusion. \n",
    "df_etCombo.rename(columns={'json': 'pt_metadata'}, inplace=True)\n",
    "\n",
    "# Function to extract stratumID\n",
    "def extract_stratumID(json_str):\n",
    "    try:\n",
    "        # Load the string as JSON\n",
    "        data = json.loads(json_str)\n",
    "        # Extract stratumID from the last entry in the list\n",
    "        stratumID = data[-1]['stratumID'] if data else None\n",
    "    except (json.JSONDecodeError, IndexError, KeyError):\n",
    "        stratumID = None\n",
    "    return stratumID\n",
    "\n",
    "# Apply the function to each row in the 'pt_metadata' column\n",
    "df_etCombo['stratumIdIntPts'] = df_etCombo['pt_metadata'].apply(extract_stratumID)\n",
    "\n",
    "# Drop the 'geo' column\n",
    "df_etCombo.drop(columns=['pt_metadata'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed8eda-575d-459a-8839-f19cc94439b3",
   "metadata": {},
   "source": [
    "#### Reorganizing the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "120cc5a5-1e12-4477-a39e-7fa4efd46a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tracker', 'longitude', 'latitude', 'stratumIdIntPts', 'reEval', 'user', 'Rec_Prob_12month_Growth', 'Rec_Prob_12month_Loss', 'Rec_Prob_12month_Stable', 'Rec_Prob_4month_Growth', 'Rec_Prob_4month_Loss', 'Rec_Prob_4month_Stable', 'Rec_Prob_6month_Growth', 'Rec_Prob_6month_Loss', 'Rec_Prob_6month_Stable', 'Rec_TF_12month_Loss', 'Rec_TF_4month_Loss', 'Rec_TF_6month_Loss', 'Ave_Prob_12month_Growth', 'Ave_Prob_12month_Loss', 'Ave_Prob_12month_Stable', 'Ave_Prob_4month_Growth', 'Ave_Prob_4month_Loss', 'Ave_Prob_4month_Stable', 'Ave_Prob_6month_Growth', 'Ave_Prob_6month_Loss', 'Ave_Prob_6month_Stable', 'Ave_TF_12month_Loss', 'Ave_TF_4month_Loss', 'Ave_TF_6month_Loss', 'E1_Prob_4month_Growth', 'E1_Prob_4month_Loss', 'E1_Prob_4month_Stable', 'E1_TF_4month_Loss', 'E1_Prob_6month_Growth', 'E1_Prob_6month_Loss', 'E1_Prob_6month_Stable', 'E1_TF_6month_Loss', 'E1_Prob_12month_Growth', 'E1_Prob_12month_Loss', 'E1_Prob_12month_Stable', 'E1_TF_12month_Loss', 'E2_Prob_4month_Growth', 'E2_Prob_4month_Loss', 'E2_Prob_4month_Stable', 'E2_TF_4month_Loss', 'E2_Prob_6month_Growth', 'E2_Prob_6month_Loss', 'E2_Prob_6month_Stable', 'E2_TF_6month_Loss', 'E2_Prob_12month_Growth', 'E2_Prob_12month_Loss', 'E2_Prob_12month_Stable', 'E2_TF_12month_Loss', 'E3_Prob_4month_Growth', 'E3_Prob_4month_Loss', 'E3_Prob_4month_Stable', 'E3_TF_4month_Loss', 'E3_Prob_6month_Growth', 'E3_Prob_6month_Loss', 'E3_Prob_6month_Stable', 'E3_TF_6month_Loss', 'E3_Prob_12month_Growth', 'E3_Prob_12month_Loss', 'E3_Prob_12month_Stable', 'E3_TF_12month_Loss', 'E4_Prob_4month_Growth', 'E4_Prob_4month_Loss', 'E4_Prob_4month_Stable', 'E4_TF_4month_Loss', 'E4_Prob_6month_Growth', 'E4_Prob_6month_Loss', 'E4_Prob_6month_Stable', 'E4_TF_6month_Loss', 'E4_Prob_12month_Growth', 'E4_Prob_12month_Loss', 'E4_Prob_12month_Stable', 'E4_TF_12month_Loss', 'E5_Prob_4month_Growth', 'E5_Prob_4month_Loss', 'E5_Prob_4month_Stable', 'E5_TF_4month_Loss', 'E5_Prob_6month_Growth', 'E5_Prob_6month_Loss', 'E5_Prob_6month_Stable', 'E5_TF_6month_Loss', 'E5_Prob_12month_Growth', 'E5_Prob_12month_Loss', 'E5_Prob_12month_Stable', 'E5_TF_12month_Loss', 'E6_Prob_4month_Growth', 'E6_Prob_4month_Loss', 'E6_Prob_4month_Stable', 'E6_TF_4month_Loss', 'E6_Prob_6month_Growth', 'E6_Prob_6month_Loss', 'E6_Prob_6month_Stable', 'E6_TF_6month_Loss', 'E6_Prob_12month_Growth', 'E6_Prob_12month_Loss', 'E6_Prob_12month_Stable', 'E6_TF_12month_Loss', 'E7_Prob_4month_Growth', 'E7_Prob_4month_Loss', 'E7_Prob_4month_Stable', 'E7_TF_4month_Loss', 'E7_Prob_6month_Growth', 'E7_Prob_6month_Loss', 'E7_Prob_6month_Stable', 'E7_TF_6month_Loss', 'E7_Prob_12month_Growth', 'E7_Prob_12month_Loss', 'E7_Prob_12month_Stable', 'E7_TF_12month_Loss', 'E8_Prob_4month_Growth', 'E8_Prob_4month_Loss', 'E8_Prob_4month_Stable', 'E8_TF_4month_Loss', 'E8_Prob_6month_Growth', 'E8_Prob_6month_Loss', 'E8_Prob_6month_Stable', 'E8_TF_6month_Loss', 'E8_Prob_12month_Growth', 'E8_Prob_12month_Loss', 'E8_Prob_12month_Stable', 'E8_TF_12month_Loss', 'E9_Prob_4month_Growth', 'E9_Prob_4month_Loss', 'E9_Prob_4month_Stable', 'E9_TF_4month_Loss', 'E9_Prob_6month_Growth', 'E9_Prob_6month_Loss', 'E9_Prob_6month_Stable', 'E9_TF_6month_Loss', 'E9_Prob_12month_Growth', 'E9_Prob_12month_Loss', 'E9_Prob_12month_Stable', 'E9_TF_12month_Loss', 'E9999_Prob_4month_Growth', 'E9999_Prob_4month_Loss', 'E9999_Prob_4month_Stable', 'E9999_TF_4month_Loss', 'E9999_Prob_6month_Growth', 'E9999_Prob_6month_Loss', 'E9999_Prob_6month_Stable', 'E9999_TF_6month_Loss', 'E9999_Prob_12month_Growth', 'E9999_Prob_12month_Loss', 'E9999_Prob_12month_Stable', 'E9999_TF_12month_Loss', 'Image_Condition', 'Spatial_Context', 'Spectral_Capacity', 'id', 'plotId', 'Valid_Name', 'Comment', 'User_IP', 'user_elapsed_time', 'Valid_Date', 'LT_YOD']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the fixed columns that should be at the start and end\n",
    "start_columns = ['tracker', 'longitude', 'latitude', 'stratumIdIntPts', 'reEval', 'user']\n",
    "end_columns = [\n",
    "    'Image_Condition', 'Spatial_Context', 'Spectral_Capacity', 'id', 'plotId',\n",
    "    'Valid_Name', 'Comment', 'User_IP', 'user_elapsed_time', 'Valid_Date', 'LT_YOD'\n",
    "]\n",
    "\n",
    "# Step 2: Extract all columns and categorize them\n",
    "all_columns = df_etCombo.columns\n",
    "\n",
    "# Helper function to safely extract numeric parts from column names\n",
    "def extract_epoch_number(col):\n",
    "    try:\n",
    "        # Extract the number part from columns like 'E999_Prob_4month_Loss'\n",
    "        return int(re.search(r'E(\\d+)', col).group(1))\n",
    "    except (AttributeError, ValueError):\n",
    "        return float('inf')  # Place non-matching columns at the end\n",
    "\n",
    "def extract_month_number(col):\n",
    "    try:\n",
    "        # Extract the month number (e.g., '4' from 'Prob_4month_Loss')\n",
    "        return int(re.search(r'(\\d+)month', col).group(1))\n",
    "    except (AttributeError, ValueError):\n",
    "        return float('inf')  # Place non-matching columns at the end\n",
    "\n",
    "# Separate columns into REC, AVE, and Epoch columns\n",
    "rec_columns = sorted(\n",
    "    [col for col in all_columns if col.startswith('Rec_')],\n",
    "    key=lambda x: (extract_epoch_number(x), x)\n",
    ")\n",
    "ave_columns = sorted(\n",
    "    [col for col in all_columns if col.startswith('Ave_')],\n",
    "    key=lambda x: (extract_epoch_number(x), x)\n",
    ")\n",
    "epoch_columns = sorted(\n",
    "    [col for col in all_columns if col.startswith('E') and ('_Prob_' in col or '_TF_' in col)],\n",
    "    key=lambda x: (extract_epoch_number(x), extract_month_number(x), x)\n",
    ")\n",
    "\n",
    "# Step 3: Combine columns in the desired order\n",
    "new_column_order = start_columns + rec_columns + ave_columns + epoch_columns + end_columns\n",
    "\n",
    "# Step 4: Ensure no columns are left out or duplicated\n",
    "missing_columns = set(all_columns) - set(new_column_order)\n",
    "if missing_columns:\n",
    "    print(\"Warning: The following columns were not included in the new order:\", missing_columns)\n",
    "    new_column_order.extend(missing_columns)\n",
    "\n",
    "# Step 5: Reorder the DataFrame columns\n",
    "df_etCombo = df_etCombo[new_column_order]\n",
    "\n",
    "print(list(df_etCombo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a31f42-4d9f-4fb0-8d36-b6763b11c9ba",
   "metadata": {},
   "source": [
    "#### Making sure everything is in integer format for RF classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2217fd3-d1a6-4586-ae15-fad63a22edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns matching the specified endings: ['Rec_Prob_12month_Growth', 'Rec_Prob_12month_Loss', 'Rec_Prob_12month_Stable', 'Rec_Prob_4month_Growth', 'Rec_Prob_4month_Loss', 'Rec_Prob_4month_Stable', 'Rec_Prob_6month_Growth', 'Rec_Prob_6month_Loss', 'Rec_Prob_6month_Stable', 'Rec_TF_12month_Loss', 'Rec_TF_4month_Loss', 'Rec_TF_6month_Loss', 'Ave_Prob_12month_Growth', 'Ave_Prob_12month_Loss', 'Ave_Prob_12month_Stable', 'Ave_Prob_4month_Growth', 'Ave_Prob_4month_Loss', 'Ave_Prob_4month_Stable', 'Ave_Prob_6month_Growth', 'Ave_Prob_6month_Loss', 'Ave_Prob_6month_Stable', 'Ave_TF_12month_Loss', 'Ave_TF_4month_Loss', 'Ave_TF_6month_Loss', 'E1_Prob_4month_Growth', 'E1_Prob_4month_Loss', 'E1_Prob_4month_Stable', 'E1_TF_4month_Loss', 'E1_Prob_6month_Growth', 'E1_Prob_6month_Loss', 'E1_Prob_6month_Stable', 'E1_TF_6month_Loss', 'E1_Prob_12month_Growth', 'E1_Prob_12month_Loss', 'E1_Prob_12month_Stable', 'E1_TF_12month_Loss', 'E2_Prob_4month_Growth', 'E2_Prob_4month_Loss', 'E2_Prob_4month_Stable', 'E2_TF_4month_Loss', 'E2_Prob_6month_Growth', 'E2_Prob_6month_Loss', 'E2_Prob_6month_Stable', 'E2_TF_6month_Loss', 'E2_Prob_12month_Growth', 'E2_Prob_12month_Loss', 'E2_Prob_12month_Stable', 'E2_TF_12month_Loss', 'E3_Prob_4month_Growth', 'E3_Prob_4month_Loss', 'E3_Prob_4month_Stable', 'E3_TF_4month_Loss', 'E3_Prob_6month_Growth', 'E3_Prob_6month_Loss', 'E3_Prob_6month_Stable', 'E3_TF_6month_Loss', 'E3_Prob_12month_Growth', 'E3_Prob_12month_Loss', 'E3_Prob_12month_Stable', 'E3_TF_12month_Loss', 'E4_Prob_4month_Growth', 'E4_Prob_4month_Loss', 'E4_Prob_4month_Stable', 'E4_TF_4month_Loss', 'E4_Prob_6month_Growth', 'E4_Prob_6month_Loss', 'E4_Prob_6month_Stable', 'E4_TF_6month_Loss', 'E4_Prob_12month_Growth', 'E4_Prob_12month_Loss', 'E4_Prob_12month_Stable', 'E4_TF_12month_Loss', 'E5_Prob_4month_Growth', 'E5_Prob_4month_Loss', 'E5_Prob_4month_Stable', 'E5_TF_4month_Loss', 'E5_Prob_6month_Growth', 'E5_Prob_6month_Loss', 'E5_Prob_6month_Stable', 'E5_TF_6month_Loss', 'E5_Prob_12month_Growth', 'E5_Prob_12month_Loss', 'E5_Prob_12month_Stable', 'E5_TF_12month_Loss', 'E6_Prob_4month_Growth', 'E6_Prob_4month_Loss', 'E6_Prob_4month_Stable', 'E6_TF_4month_Loss', 'E6_Prob_6month_Growth', 'E6_Prob_6month_Loss', 'E6_Prob_6month_Stable', 'E6_TF_6month_Loss', 'E6_Prob_12month_Growth', 'E6_Prob_12month_Loss', 'E6_Prob_12month_Stable', 'E6_TF_12month_Loss', 'E7_Prob_4month_Growth', 'E7_Prob_4month_Loss', 'E7_Prob_4month_Stable', 'E7_TF_4month_Loss', 'E7_Prob_6month_Growth', 'E7_Prob_6month_Loss', 'E7_Prob_6month_Stable', 'E7_TF_6month_Loss', 'E7_Prob_12month_Growth', 'E7_Prob_12month_Loss', 'E7_Prob_12month_Stable', 'E7_TF_12month_Loss', 'E8_Prob_4month_Growth', 'E8_Prob_4month_Loss', 'E8_Prob_4month_Stable', 'E8_TF_4month_Loss', 'E8_Prob_6month_Growth', 'E8_Prob_6month_Loss', 'E8_Prob_6month_Stable', 'E8_TF_6month_Loss', 'E8_Prob_12month_Growth', 'E8_Prob_12month_Loss', 'E8_Prob_12month_Stable', 'E8_TF_12month_Loss', 'E9_Prob_4month_Growth', 'E9_Prob_4month_Loss', 'E9_Prob_4month_Stable', 'E9_TF_4month_Loss', 'E9_Prob_6month_Growth', 'E9_Prob_6month_Loss', 'E9_Prob_6month_Stable', 'E9_TF_6month_Loss', 'E9_Prob_12month_Growth', 'E9_Prob_12month_Loss', 'E9_Prob_12month_Stable', 'E9_TF_12month_Loss', 'E9999_Prob_4month_Growth', 'E9999_Prob_4month_Loss', 'E9999_Prob_4month_Stable', 'E9999_TF_4month_Loss', 'E9999_Prob_6month_Growth', 'E9999_Prob_6month_Loss', 'E9999_Prob_6month_Stable', 'E9999_TF_6month_Loss', 'E9999_Prob_12month_Growth', 'E9999_Prob_12month_Loss', 'E9999_Prob_12month_Stable', 'E9999_TF_12month_Loss']\n",
      "Non-integer numeric columns: ['Ave_Prob_12month_Growth', 'Ave_Prob_12month_Loss', 'Ave_Prob_12month_Stable', 'Ave_Prob_4month_Growth', 'Ave_Prob_4month_Loss', 'Ave_Prob_4month_Stable', 'Ave_Prob_6month_Growth', 'Ave_Prob_6month_Loss', 'Ave_Prob_6month_Stable']\n"
     ]
    }
   ],
   "source": [
    "# Build regex pattern to match endings like '4month_Loss', '12month_Stable', etc.\n",
    "pattern = re.compile(r'\\d*month_(Loss|Stable|Growth)$')\n",
    "\n",
    "# Filter columns matching the pattern\n",
    "allDependentColumns = [col for col in df_etCombo.columns if pattern.search(col)]\n",
    "\n",
    "# Print the matching columns\n",
    "print(\"Columns matching the specified endings:\", allDependentColumns)\n",
    "\n",
    "# List to store columns that are numeric but not integer\n",
    "non_integer_numeric_cols = []\n",
    "\n",
    "for col in allDependentColumns:\n",
    "    if pd.api.types.is_numeric_dtype(df_etCombo[col]):\n",
    "        if not np.all(df_etCombo[col].dropna() == df_etCombo[col].dropna().astype(int)):\n",
    "            non_integer_numeric_cols.append(col)\n",
    "\n",
    "print(\"Non-integer numeric columns:\", non_integer_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f6649d6-53cd-4fef-a982-501367deafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_integer_numeric_cols:\n",
    "    df_etCombo[col] = df_etCombo[col].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c8b75-00e9-401a-abd7-3f9ad5a155aa",
   "metadata": {},
   "source": [
    "#### Exporting the full CSV (user observations still separate for a single point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba1826f2-8086-4daf-a139-0eca36229d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTING!!!!!\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_etCombo.to_csv('camIntObservationsFull.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2893e2-dc73-46a8-91cd-f1dbac46b455",
   "metadata": {},
   "source": [
    "#### Formatting for GEE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd2c4717-d243-4e9c-9b41-755c99d08b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tracker', 'latitude', 'longitude', 'user', 'Rec_Prob_12month_Loss', 'Rec_TF_12month_Loss', 'Ave_Prob_12month_Loss', 'Ave_TF_12month_Loss']\n"
     ]
    }
   ],
   "source": [
    "# Export GEE CSV with separate users for repeated points. \n",
    "\n",
    "# EXPORTING!!!!!\n",
    "\n",
    "#Create the new DataFrame with selected columns\n",
    "df_etComboGeeSeparate = df_etCombo[['tracker', 'latitude', 'longitude', 'user', 'Rec_Prob_12month_Loss', 'Rec_TF_12month_Loss', 'Ave_Prob_12month_Loss', 'Ave_TF_12month_Loss']].copy()\n",
    "\n",
    "print(list(df_etComboGeeSeparate))\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_etComboGeeSeparate.to_csv('camIntObservationsGee_SeparateUsers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fc9d675-87a1-4f13-8483-14fb4af19546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new DataFrame called 'df_etComboGee'.\n"
     ]
    }
   ],
   "source": [
    "#Create the new DataFrame with selected columns\n",
    "df_etComboGee = df_etCombo[['tracker', 'latitude', 'longitude', 'user']].copy()\n",
    "\n",
    "# Drop the user column since we are changing the format for the other GEE file. \n",
    "df_etComboGee = df_etComboGee.drop('user', axis=1)\n",
    "\n",
    "print(\"Created a new DataFrame called 'df_etComboGee'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "198ccc6c-da46-464e-940f-ab949613bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in 'df_etComboGee':\n",
      "['tracker', 'latitude', 'longitude', 'user1', 'user2', 'Rec_AveProb_4month_Loss', 'Rec_AveProb_4month_Stable', 'Rec_AveProb_4month_Growth', 'Rec_AveProb_6month_Loss', 'Rec_AveProb_6month_Stable', 'Rec_AveProb_6month_Growth', 'Rec_AveProb_12month_Loss', 'Rec_AveProb_12month_Stable', 'Rec_AveProb_12month_Growth']\n",
      "Total number of rows: 2617\n"
     ]
    }
   ],
   "source": [
    "# Add new columns needed for GEE. \n",
    "\n",
    "# Add 'user1' and 'user2' columns to the new DataFrame\n",
    "df_etComboGee['user1'] = np.nan\n",
    "df_etComboGee['user2'] = np.nan\n",
    "\n",
    "# Define the month and condition options\n",
    "months = [4, 6, 12]\n",
    "conditions = ['Loss', 'Stable', 'Growth']\n",
    "\n",
    "# Generate all combinations for the new columns and add them to the DataFrame\n",
    "new_columns = [f'Rec_AveProb_{month}month_{condition}' for month in months for condition in conditions]\n",
    "\n",
    "# Add these columns to the DataFrame with NaN values\n",
    "for col in new_columns:\n",
    "    df_etComboGee[col] = np.nan\n",
    "\n",
    "# Print the list of columns in the new DataFrame\n",
    "print(\"\\nColumns in 'df_etComboGee':\")\n",
    "print(df_etComboGee.columns.tolist())\n",
    "\n",
    "# Count the total number of rows in the DataFrame\n",
    "total_rows = df_etComboGee.shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3c1eb98-9095-40dd-80fa-2f8775d83d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed rows with duplicate 'tracker' values from 'df_etComboGee'.\n",
      "New shape of 'df_etComboGee': (2031, 14)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with duplicate 'tracker' values, keeping only the first occurrence\n",
    "df_etComboGee = df_etComboGee.drop_duplicates(subset=['tracker'], keep='first')\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Removed rows with duplicate 'tracker' values from 'df_etComboGee'.\")\n",
    "print(f\"New shape of 'df_etComboGee': {df_etComboGee.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f545a0d-db58-4e15-843b-79d2e607e495",
   "metadata": {},
   "source": [
    "##### Combining all observations for each point for training. (Some points have multiple interpreters look at them which will not work for training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1caf5f2-5eb5-4caa-be9f-42ef84130921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed filling user1 and user2 columns.\n",
      "    tracker   latitude   longitude  user1  user2  Rec_AveProb_4month_Loss  \\\n",
      "0  87950922  12.127876  103.841873    1.0    2.0                      NaN   \n",
      "1  81395819  12.983627  107.085125    1.0    3.0                      NaN   \n",
      "2  84152401  13.678627  104.668876    1.0    4.0                      NaN   \n",
      "3  66411237  11.563377  105.789876    1.0    5.0                      NaN   \n",
      "4  29398292  14.220876  104.229873    1.0    6.0                      NaN   \n",
      "\n",
      "   Rec_AveProb_4month_Stable  Rec_AveProb_4month_Growth  \\\n",
      "0                        NaN                        NaN   \n",
      "1                        NaN                        NaN   \n",
      "2                        NaN                        NaN   \n",
      "3                        NaN                        NaN   \n",
      "4                        NaN                        NaN   \n",
      "\n",
      "   Rec_AveProb_6month_Loss  Rec_AveProb_6month_Stable  \\\n",
      "0                      NaN                        NaN   \n",
      "1                      NaN                        NaN   \n",
      "2                      NaN                        NaN   \n",
      "3                      NaN                        NaN   \n",
      "4                      NaN                        NaN   \n",
      "\n",
      "   Rec_AveProb_6month_Growth  Rec_AveProb_12month_Loss  \\\n",
      "0                        NaN                       NaN   \n",
      "1                        NaN                       NaN   \n",
      "2                        NaN                       NaN   \n",
      "3                        NaN                       NaN   \n",
      "4                        NaN                       NaN   \n",
      "\n",
      "   Rec_AveProb_12month_Stable  Rec_AveProb_12month_Growth  \n",
      "0                         NaN                         NaN  \n",
      "1                         NaN                         NaN  \n",
      "2                         NaN                         NaN  \n",
      "3                         NaN                         NaN  \n",
      "4                         NaN                         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Filling the 'user1' and 'user2' columns. \n",
    "# First, initialize user1 and user2 columns with NaN values\n",
    "df_etComboGee['user1'] = np.nan\n",
    "df_etComboGee['user2'] = np.nan\n",
    "\n",
    "# Iterate over the new DataFrame to fill the user1 and user2 columns\n",
    "for index, row in df_etComboGee.iterrows():\n",
    "    tracker_value = row['tracker']\n",
    "\n",
    "    # Find all matching rows in the original DataFrame based on the tracker value\n",
    "    matching_rows = df_etCombo[df_etCombo['tracker'] == tracker_value]\n",
    "\n",
    "    # Extract user numbers from matching rows\n",
    "    user_values = matching_rows['user'].unique()\n",
    "\n",
    "    # Fill the user1 and user2 columns if available\n",
    "    if len(user_values) >= 1:\n",
    "        df_etComboGee.at[index, 'user1'] = user_values[0]\n",
    "    if len(user_values) >= 2:\n",
    "        df_etComboGee.at[index, 'user2'] = user_values[1]\n",
    "\n",
    "    # Print tracker and user values if there are more than 2 matching users\n",
    "    if len(user_values) > 2:\n",
    "        print(f\"Tracker {tracker_value} has more than two users:\")\n",
    "        for i, user in enumerate(user_values[2:], start=3):\n",
    "            print(f\"User {i}: {user}\")\n",
    "\n",
    "# Print confirmation of completion\n",
    "print(\"\\nCompleted filling user1 and user2 columns.\")\n",
    "print(df_etComboGee.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c2973e3-e3c3-46dc-b883-ef197e9e7458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 2031\n",
      "Total number of unique tracker numbers: 2031\n",
      "Number of rows with a value in the 'user2' column: 583\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows in the DataFrame\n",
    "total_rows = df_etComboGee.shape[0]\n",
    "\n",
    "# Count the total number of unique tracker numbers\n",
    "unique_trackers = df_etComboGee['tracker'].nunique()\n",
    "\n",
    "# Count the number of rows where 'user2' is not NaN\n",
    "user2_count = df_etComboGee['user2'].notna().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of unique tracker numbers: {unique_trackers}\")\n",
    "print(f\"Number of rows with a value in the 'user2' column: {user2_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a62a36d3-92d8-40b5-bd98-ff610f4def21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed filling Rec_AveProb_<MM>month_<condition> columns.\n",
      "    tracker   latitude   longitude  user1  user2  Rec_AveProb_4month_Loss  \\\n",
      "0  87950922  12.127876  103.841873    1.0    2.0                      0.0   \n",
      "1  81395819  12.983627  107.085125    1.0    3.0                     75.0   \n",
      "2  84152401  13.678627  104.668876    1.0    4.0                      2.5   \n",
      "3  66411237  11.563377  105.789876    1.0    5.0                      0.0   \n",
      "4  29398292  14.220876  104.229873    1.0    6.0                      2.5   \n",
      "\n",
      "   Rec_AveProb_4month_Stable  Rec_AveProb_4month_Growth  \\\n",
      "0                      100.0                        0.0   \n",
      "1                       25.0                        0.0   \n",
      "2                       97.5                        0.0   \n",
      "3                      100.0                        0.0   \n",
      "4                       97.5                        0.0   \n",
      "\n",
      "   Rec_AveProb_6month_Loss  Rec_AveProb_6month_Stable  \\\n",
      "0                      0.0                      100.0   \n",
      "1                     30.0                       70.0   \n",
      "2                      2.5                       97.5   \n",
      "3                      0.0                      100.0   \n",
      "4                     12.5                       87.5   \n",
      "\n",
      "   Rec_AveProb_6month_Growth  Rec_AveProb_12month_Loss  \\\n",
      "0                        0.0                       0.0   \n",
      "1                        0.0                      87.5   \n",
      "2                        0.0                       2.5   \n",
      "3                        0.0                       0.0   \n",
      "4                        0.0                      15.0   \n",
      "\n",
      "   Rec_AveProb_12month_Stable  Rec_AveProb_12month_Growth  \n",
      "0                       100.0                         0.0  \n",
      "1                        12.5                         0.0  \n",
      "2                        97.5                         0.0  \n",
      "3                       100.0                         0.0  \n",
      "4                        85.0                         0.0  \n"
     ]
    }
   ],
   "source": [
    "# Iterate through all Rec_AveProb_<MM>month_<condition> columns in the new DataFrame\n",
    "for month in months:\n",
    "    for condition in conditions:\n",
    "        # Define the corresponding column name in the new DataFrame\n",
    "        new_col_name = f'Rec_AveProb_{month}month_{condition}'\n",
    "        \n",
    "        # Define the corresponding column name in the original DataFrame\n",
    "        orig_col_name = f'Rec_Prob_{month}month_{condition}'\n",
    "\n",
    "        # Ensure the original column exists before proceeding\n",
    "        if orig_col_name in df_etCombo.columns:\n",
    "            # Iterate over each row in the new DataFrame\n",
    "            for index, row in df_etComboGee.iterrows():\n",
    "                tracker_value = row['tracker']\n",
    "\n",
    "                # Find matching rows in the original DataFrame based on the tracker number\n",
    "                matching_rows = df_etCombo[df_etCombo['tracker'] == tracker_value]\n",
    "\n",
    "                if not matching_rows.empty:\n",
    "                    # Calculate the average value of the matching rows for the corresponding column\n",
    "                    avg_value = matching_rows[orig_col_name].mean()\n",
    "\n",
    "                    # Assign the average value to the new DataFrame\n",
    "                    df_etComboGee.at[index, new_col_name] = avg_value\n",
    "\n",
    "# Print confirmation of completion\n",
    "print(\"\\nCompleted filling Rec_AveProb_<MM>month_<condition> columns.\")\n",
    "print(df_etComboGee.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31e58f-8663-4fc6-b9e1-8a10d07c8be3",
   "metadata": {},
   "source": [
    "##### Create TF columns for the new combined probability values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8db4ee83-5595-41b7-9960-40b9361120ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added and populated Rec_AveTF_<MM>month_<condition> columns.\n",
      "    tracker   latitude   longitude  user1  user2  Rec_AveProb_4month_Loss  \\\n",
      "0  87950922  12.127876  103.841873    1.0    2.0                      0.0   \n",
      "1  81395819  12.983627  107.085125    1.0    3.0                     75.0   \n",
      "2  84152401  13.678627  104.668876    1.0    4.0                      2.5   \n",
      "3  66411237  11.563377  105.789876    1.0    5.0                      0.0   \n",
      "4  29398292  14.220876  104.229873    1.0    6.0                      2.5   \n",
      "\n",
      "   Rec_AveProb_4month_Stable  Rec_AveProb_4month_Growth  \\\n",
      "0                      100.0                        0.0   \n",
      "1                       25.0                        0.0   \n",
      "2                       97.5                        0.0   \n",
      "3                      100.0                        0.0   \n",
      "4                       97.5                        0.0   \n",
      "\n",
      "   Rec_AveProb_6month_Loss  Rec_AveProb_6month_Stable  ...  \\\n",
      "0                      0.0                      100.0  ...   \n",
      "1                     30.0                       70.0  ...   \n",
      "2                      2.5                       97.5  ...   \n",
      "3                      0.0                      100.0  ...   \n",
      "4                     12.5                       87.5  ...   \n",
      "\n",
      "   Rec_AveProb_12month_Growth  Rec_AveTF_4month_Loss  Rec_AveTF_4month_Stable  \\\n",
      "0                         0.0                      0                        1   \n",
      "1                         0.0                      1                        0   \n",
      "2                         0.0                      0                        1   \n",
      "3                         0.0                      0                        1   \n",
      "4                         0.0                      0                        1   \n",
      "\n",
      "   Rec_AveTF_4month_Growth  Rec_AveTF_6month_Loss  Rec_AveTF_6month_Stable  \\\n",
      "0                        0                      0                        1   \n",
      "1                        0                      0                        1   \n",
      "2                        0                      0                        1   \n",
      "3                        0                      0                        1   \n",
      "4                        0                      0                        1   \n",
      "\n",
      "   Rec_AveTF_6month_Growth  Rec_AveTF_12month_Loss  Rec_AveTF_12month_Stable  \\\n",
      "0                        0                       0                         1   \n",
      "1                        0                       1                         0   \n",
      "2                        0                       0                         1   \n",
      "3                        0                       0                         1   \n",
      "4                        0                       0                         1   \n",
      "\n",
      "   Rec_AveTF_12month_Growth  \n",
      "0                         0  \n",
      "1                         0  \n",
      "2                         0  \n",
      "3                         0  \n",
      "4                         0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all combinations of month and condition\n",
    "for month in months:\n",
    "    for condition in conditions:\n",
    "        # Define the corresponding AveProb column name\n",
    "        prob_col = f'Rec_AveProb_{month}month_{condition}'\n",
    "\n",
    "        # Define the new AveTF column name\n",
    "        tf_col = f'Rec_AveTF_{month}month_{condition}'\n",
    "\n",
    "        # Ensure the AveProb column exists in the DataFrame\n",
    "        if prob_col in df_etComboGee.columns:\n",
    "            # Create the new AveTF column based on the value in the AveProb column\n",
    "            df_etComboGee[tf_col] = np.where(\n",
    "                df_etComboGee[prob_col] > 50.0, 1, 0\n",
    "            )\n",
    "\n",
    "# Print confirmation of completion\n",
    "print(\"\\nAdded and populated Rec_AveTF_<MM>month_<condition> columns.\")\n",
    "print(df_etComboGee.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16907e12-c099-4ecc-b856-b0611edf55eb",
   "metadata": {},
   "source": [
    "##### Export the dataframe for GEE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f9f454b-973f-4e82-ab9f-16dcc158585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tracker', 'latitude', 'longitude', 'user1', 'user2', 'Rec_AveProb_4month_Loss', 'Rec_AveProb_4month_Stable', 'Rec_AveProb_4month_Growth', 'Rec_AveProb_6month_Loss', 'Rec_AveProb_6month_Stable', 'Rec_AveProb_6month_Growth', 'Rec_AveProb_12month_Loss', 'Rec_AveProb_12month_Stable', 'Rec_AveProb_12month_Growth', 'Rec_AveTF_4month_Loss', 'Rec_AveTF_4month_Stable', 'Rec_AveTF_4month_Growth', 'Rec_AveTF_6month_Loss', 'Rec_AveTF_6month_Stable', 'Rec_AveTF_6month_Growth', 'Rec_AveTF_12month_Loss', 'Rec_AveTF_12month_Stable', 'Rec_AveTF_12month_Growth']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_etComboGee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04667b1c-85e2-4ca1-9c06-522173c6f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a new CSV file\n",
    "df_etComboGee.to_csv('camIntObservationsGee_CombinedByTracker.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06776b0d-02ab-4cd6-8bd6-e1ab8eaa751e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
